# -*- coding: utf-8 -*-
"""LM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12B1sfBuWhcULmdCGg99UqcXgc-ael0XB
"""

import numpy as np
import pandas as pd
from tensorflow.keras.utils import to_categorical
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import LSTM, Dense, GRU, Embedding
from keras.callbacks import EarlyStopping, ModelCheckpoint
import re
from numpy import array
from pickle import dump
import string
from random import randint
from pickle import load
from keras.models import load_model
from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer

# generate a sequence from a language model
def generate_seq(model, tokenizer, seq_length, seed_text, n_words):
	result = list()
	in_text = seed_text
	# generate a fixed number of words
	for _ in range(n_words):
		# encode the text as integer
		encoded = tokenizer.texts_to_sequences([in_text])[0]
		# truncate sequences to a fixed length
		encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')
		# predict probabilities for each word
		yhat = model.predict_classes(encoded, verbose=0)
		# map predicted word index to word
		out_word = ''
		for word, index in tokenizer.word_index.items():
			if index == yhat:
				out_word = word
				break
		# append to input
		in_text += ' ' + out_word
		result.append(out_word)
	return ' '.join(result)

"""class 1(non motiv)"""

combined_data = pd.read_csv('../../data/process/combined_data_word_broken.csv')
data_text  = ""
tokens=[]
print(combined_data['text'][0].split() )
for i in range(combined_data.shape[0]):
  if combined_data['class'][i] == 1:
    data_text  += combined_data['text'][i] 
    for w in combined_data['text'][i].split():
      tokens.append(w)

print(np.shape(tokens))
print(tokens[0])

print(tokens[:200])
print('Total Tokens: %d' % len(tokens))
print('Unique Tokens: %d' % len(set(tokens)))

length = 50 + 1
sequences = list()
for i in range(length, len(tokens)):
	# select sequence of tokens
	seq = tokens[i-length:i]
	# convert into a line
	line = ' '.join(seq)
	# store
	sequences.append(line)
print('Total Sequences: %d' % len(sequences))

lines = sequences

# integer encode sequences of words
tokenizer = Tokenizer()
tokenizer.fit_on_texts(lines)
sequences = tokenizer.texts_to_sequences(lines)

vocab_size = len(tokenizer.word_index) + 1

# separate into input and output
sequences = array(sequences)
X, y = sequences[:,:-1], sequences[:,-1]
y = to_categorical(y, num_classes=vocab_size)
seq_length = X.shape[1]

model = Sequential()
model.add(Embedding(vocab_size, 50, input_length=seq_length))
model.add(LSTM(100, return_sequences=True))
model.add(LSTM(100))
model.add(Dense(100, activation='relu'))
model.add(Dense(vocab_size, activation='softmax'))
print(model.summary())

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
# fit model
model.fit(X, y, batch_size=128, epochs=100)
 
# save the model to file
model.save('nonMotivational.language_model')

dump(tokenizer, open('tokenizer.pkl', 'wb'))

# load the model
model = load_model('nonMotivational.language_model')

# load the tokenizer
tokenizer = load(open('tokenizer.pkl', 'rb'))

seed_text ='the war between syria and ISIS has affected life of many muslims'
generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)

print(seed_text+" "+generated)

"""class 0(motiv)"""

combined_data = pd.read_csv('../../data/process/combined_data_word_broken.csv')
data_text  = ""
tokens=[]
for i in range(combined_data.shape[0]):
  if combined_data['class'][i] == 0:
    data_text  += combined_data['text'][i]
    for w in combined_data['text'][i].split():
      tokens.append(w) 

print(np.shape(tokens))
print(tokens[0])

# clean document
print(tokens[:200])
print('Total Tokens: %d' % len(tokens))
print('Unique Tokens: %d' % len(set(tokens)))

length = 50 + 1
sequences = list()
for i in range(length, len(tokens)):
	# select sequence of tokens
	seq = tokens[i-length:i]
	# convert into a line
	line = ' '.join(seq)
	# store
	sequences.append(line)
print('Total Sequences: %d' % len(sequences))

lines = sequences

# integer encode sequences of words
tokenizer = Tokenizer()
tokenizer.fit_on_texts(lines)
sequences = tokenizer.texts_to_sequences(lines)

vocab_size = len(tokenizer.word_index) + 1

# separate into input and output
sequences = array(sequences)
X, y = sequences[:,:-1], sequences[:,-1]
y = to_categorical(y, num_classes=vocab_size)
seq_length = X.shape[1]

model = Sequential()
model.add(Embedding(vocab_size, 50, input_length=seq_length))
model.add(LSTM(100, return_sequences=True))
model.add(LSTM(100))
model.add(Dense(100, activation='relu'))
model.add(Dense(vocab_size, activation='softmax'))
print(model.summary())

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
# fit model
model.fit(X, y, batch_size=128, epochs=100)
 
# save the model to file
model.save('motivational.language_model')

dump(tokenizer, open('tokenizer_0.pkl', 'wb'))

# load the model
model = load_model('motivational.language_model')

# load the tokenizer
tokenizer = load(open('tokenizer_0.pkl', 'rb'))

# select a seed text
seed_text = "to be productive you should start"
# seed_text = "for being productive"
print(seed_text + '\n')

# generate new text
generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)

# print(generated)

print(seed_text+" "+generated)

# from google.colab import drive
# drive.mount('/content/gdrive')
# !cp "/content/motivational.language_model" "content/gdrive/My Drive/models"

# !cp -r "/content/motivational.language_model" /content/gdrive/MyDrive/lm_model/

# !cp -r "/content/nonMotivational.language_model" /content/gdrive/MyDrive/lm_model/

