{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa1DEm2hWFjz"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, GRU, Embedding\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import re\n",
        "from numpy import array\n",
        "from pickle import dump\n",
        "import string\n",
        "from random import randint\n",
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq9MiEJU4DM5"
      },
      "source": [
        "# generate a sequence from a language model\n",
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "\tresult = list()\n",
        "\tin_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "\tfor _ in range(n_words):\n",
        "\t\t# encode the text as integer\n",
        "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\t\t# predict probabilities for each word\n",
        "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
        "\t\t# map predicted word index to word\n",
        "\t\tout_word = ''\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += ' ' + out_word\n",
        "\t\tresult.append(out_word)\n",
        "\treturn ' '.join(result)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jKrGqRTN4tL"
      },
      "source": [
        "class 1(non motiv)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8kS6H9WN6cv",
        "outputId": "ae94d382-e787-4814-daa0-5e65d67df5c1"
      },
      "source": [
        "combined_data = pd.read_csv('combined_data_word_broken.csv')\n",
        "data_text  = \"\"\n",
        "tokens=[]\n",
        "print(combined_data['text'][0].split() )\n",
        "for i in range(combined_data.shape[0]):\n",
        "  if combined_data['class'][i] == 1:\n",
        "    data_text  += combined_data['text'][i] \n",
        "    for w in combined_data['text'][i].split():\n",
        "      tokens.append(w)\n",
        "\n",
        "print(np.shape(tokens))\n",
        "print(tokens[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['greatest', 'talent', 'always', 'coming', 'excuse', 'putting', 'something', 'want', 'skipped', 'make', 'tomorrow', 'snooze', 'button', 'well', 'needed', 'rest', 'brushed', 'task', 'todo', 'list', 'important', 'procrastination', 'always', 'inner', 'battle', 'losing', 'greater', 'consequence', 'missed', 'workout', 'blown', 'deadline', 'every', 'year', 'tried', 'pursue', 'dream', 'becoming', 'entrepreneur', 'freelance', 'writing', 'built', 'website', 'took', 'content', 'marketing', 'design', 'project', 'despite', 'time', 'seeing', 'result', 'wanted', 'uncomfortable', 'work', 'difficult', 'tedious', 'unsatisfying', 'task', 'come', 'actually', 'building', 'business', 'kept', 'looking', 'relief', 'task', 'often', 'scrolling', 'news', 'going', 'coffee', 'break', 'degree', 'still', 'story', 'life', 'right', 'writing', 'book', 'writing', 'hard', 'keep', 'fighting', 'urge', 'work', 'moving', 'along', 'getting', 'done', 'difference', 'life', 'year', 'life', 'today', 'system', 'forgemediumcom', 'mean', 'truly', 'productive', 'rely', 'hack', 'apps', 'technology', 'trust', 'tried', 'need', 'develop', 'sustainable', 'system', 'routine', 'practice', 'serf', 'foundation', 'work', 'system', 'look', 'like', 'noticed', 'thing', 'related', 'work', 'directly', 'okay', 'system', 'right', 'headspace', 'work', 'done', 'make', 'disciplined', 'person', 'write', 'power', 'distracting', 'thought', 'actually', 'write', 'always', 'want', 'procrastinate', 'system', 'productive', 'spite', 'work', 'creating', 'today', 'tomorrow']\n",
            "(66403,)\n",
            "sometimes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgEJcTmyObmr",
        "outputId": "7d7d01df-e0d6-45c7-e298-4c8e898c2c73"
      },
      "source": [
        "print(tokens[:200])\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['sometimes', 'wish', 'could', 'gather', 'woman', 'ever', 'known', 'encountered', 'conduct', 'informal', 'poll', 'raise', 'hand', 'ever', 'behaved', 'badly', 'blamed', 'period', 'raise', 'hand', 'ever', 'acted', 'helpless', 'face', 'unpleasantifnotphysicallydemanding', 'task', 'like', 'dealing', 'wild', 'animal', 'gotten', 'inside', 'house', 'raise', 'hand', 'ever', 'coerced', 'even', 'though', 'seem', 'really', 'want', 'raise', 'hand', 'thought', 'liberty', 'coercing', 'always', 'want', 'feel', 'lucky', 'time', 'raise', 'hand', 'ever', 'threatened', 'harm', 'break', 'want', 'anymore', 'raise', 'hand', 'physically', 'abusive', 'male', 'partner', 'knowing', 'unlikely', 'face', 'legal', 'consequence', 'raise', 'hand', 'lied', 'birth', 'control', 'faked', 'pregnancy', 'scare', 'would', 'respond', 'raise', 'hand', 'ever', 'manipulated', 'divorce', 'child', 'custody', 'dispute', 'favor', 'falsely', 'insinuating', 'abusive', 'toward', 'child', 'hypothetical', 'gathering', 'every', 'woman', 'ever', 'known', 'encountered', 'imagining', 'football', 'stadium', 'decent', 'capacity', 'certain', 'single', 'question', 'answered', 'honestly', 'send', 'hand', 'including', 'know', 'guilty', 'pest', 'control', 'front', 'want', 'think', 'hard', 'others', 'hear', 'much', 'toxic', 'masculinity', 'amorphous', 'term', 'refers', 'trait', 'like', 'aggression', 'emotional', 'repression', 'baked', 'male', 'social', 'norm', 'also', 'frequently', 'show', 'online', 'feminism', 'lazy', 'shorthand', 'registering', 'disapproval', 'anything', 'going', 'grant', 'equal', 'right', 'woman', 'admit', 'toxic', 'femininity', 'also', 'exists', 'poisonous', 'minor', 'form', 'feminine', 'toxin', 'like', 'blaming', 'irrational', 'temper', 'tantrum', 'hormonal', 'feigning', 'helplessness', 'order', 'want', 'major', 'toxin', 'many', 'weaponizing', 'fragility', 'cause', 'harm', 'difficult', 'time', 'defending', 'lest', 'look', 'like', 'aggressor', 'woman', 'course', 'unleash', 'tactic', 'woman', 'romantic', 'partner', 'sake', 'discussion', 'talking', 'woman']\n",
            "Total Tokens: 66403\n",
            "Unique Tokens: 12031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SifYg02TOrch",
        "outputId": "26ab79f3-6c5a-46ef-c407-55344fa32567"
      },
      "source": [
        "length = 50 + 1\n",
        "sequences = list()\n",
        "for i in range(length, len(tokens)):\n",
        "\t# select sequence of tokens\n",
        "\tseq = tokens[i-length:i]\n",
        "\t# convert into a line\n",
        "\tline = ' '.join(seq)\n",
        "\t# store\n",
        "\tsequences.append(line)\n",
        "print('Total Sequences: %d' % len(sequences))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Sequences: 66352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAOgeWfSPFKI"
      },
      "source": [
        "lines = sequences"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SVqP4eEPIIB"
      },
      "source": [
        "# integer encode sequences of words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x64qwgjuPK5G"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5VwQxUIPbHU"
      },
      "source": [
        "# separate into input and output\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EB7W5VQKRI6j",
        "outputId": "a7c29e2e-c240-483e-9ca2-cb6115f66656"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 50, 50)            601600    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 50, 100)           60400     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 12032)             1215232   \n",
            "=================================================================\n",
            "Total params: 1,967,732\n",
            "Trainable params: 1,967,732\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtW0HprfRQ3m",
        "outputId": "7bb57909-a973-410f-a0d4-13479ce64738"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "model.fit(X, y, batch_size=128, epochs=100)\n",
        " \n",
        "# save the model to file\n",
        "model.save('nonMotivational.language_model')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "519/519 [==============================] - 36s 26ms/step - loss: 8.7341 - accuracy: 0.0080\n",
            "Epoch 2/100\n",
            "519/519 [==============================] - 14s 26ms/step - loss: 8.2286 - accuracy: 0.0081\n",
            "Epoch 3/100\n",
            "519/519 [==============================] - 13s 26ms/step - loss: 8.1065 - accuracy: 0.0095\n",
            "Epoch 4/100\n",
            "519/519 [==============================] - 14s 26ms/step - loss: 8.0283 - accuracy: 0.0091\n",
            "Epoch 5/100\n",
            "519/519 [==============================] - 13s 26ms/step - loss: 7.9108 - accuracy: 0.0102\n",
            "Epoch 6/100\n",
            "519/519 [==============================] - 13s 26ms/step - loss: 7.7843 - accuracy: 0.0097\n",
            "Epoch 7/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 7.7466 - accuracy: 0.0103\n",
            "Epoch 8/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 7.5879 - accuracy: 0.0117\n",
            "Epoch 9/100\n",
            "519/519 [==============================] - 13s 26ms/step - loss: 7.4655 - accuracy: 0.0122\n",
            "Epoch 10/100\n",
            "519/519 [==============================] - 13s 26ms/step - loss: 7.3287 - accuracy: 0.0144\n",
            "Epoch 11/100\n",
            "519/519 [==============================] - 14s 27ms/step - loss: 7.2027 - accuracy: 0.0142\n",
            "Epoch 12/100\n",
            "519/519 [==============================] - 14s 26ms/step - loss: 7.0590 - accuracy: 0.0161\n",
            "Epoch 13/100\n",
            "519/519 [==============================] - 14s 26ms/step - loss: 6.9142 - accuracy: 0.0169\n",
            "Epoch 14/100\n",
            "519/519 [==============================] - 13s 26ms/step - loss: 6.7719 - accuracy: 0.0181\n",
            "Epoch 15/100\n",
            "519/519 [==============================] - 14s 26ms/step - loss: 6.6138 - accuracy: 0.0197\n",
            "Epoch 16/100\n",
            "519/519 [==============================] - 13s 26ms/step - loss: 6.4577 - accuracy: 0.0216\n",
            "Epoch 17/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 6.3098 - accuracy: 0.0252\n",
            "Epoch 18/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 6.1564 - accuracy: 0.0274\n",
            "Epoch 19/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 5.9945 - accuracy: 0.0335\n",
            "Epoch 20/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 5.8474 - accuracy: 0.0426\n",
            "Epoch 21/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 5.7060 - accuracy: 0.0497\n",
            "Epoch 22/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 5.5677 - accuracy: 0.0620\n",
            "Epoch 23/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 5.4721 - accuracy: 0.0760\n",
            "Epoch 24/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 5.3416 - accuracy: 0.0866\n",
            "Epoch 25/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 5.2623 - accuracy: 0.0976\n",
            "Epoch 26/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 5.0988 - accuracy: 0.1124\n",
            "Epoch 27/100\n",
            "519/519 [==============================] - 13s 26ms/step - loss: 4.9932 - accuracy: 0.1238\n",
            "Epoch 28/100\n",
            "519/519 [==============================] - 14s 26ms/step - loss: 4.8790 - accuracy: 0.1395\n",
            "Epoch 29/100\n",
            "519/519 [==============================] - 13s 26ms/step - loss: 4.7800 - accuracy: 0.1510\n",
            "Epoch 30/100\n",
            "519/519 [==============================] - 13s 26ms/step - loss: 4.6714 - accuracy: 0.1631\n",
            "Epoch 31/100\n",
            "519/519 [==============================] - 14s 26ms/step - loss: 4.5829 - accuracy: 0.1771\n",
            "Epoch 32/100\n",
            "519/519 [==============================] - 13s 26ms/step - loss: 4.5019 - accuracy: 0.1846\n",
            "Epoch 33/100\n",
            "519/519 [==============================] - 14s 27ms/step - loss: 4.4081 - accuracy: 0.1982\n",
            "Epoch 34/100\n",
            "519/519 [==============================] - 14s 27ms/step - loss: 4.3231 - accuracy: 0.2104\n",
            "Epoch 35/100\n",
            "519/519 [==============================] - 14s 27ms/step - loss: 4.2666 - accuracy: 0.2172\n",
            "Epoch 36/100\n",
            "519/519 [==============================] - 13s 26ms/step - loss: 4.1829 - accuracy: 0.2261\n",
            "Epoch 37/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 4.1092 - accuracy: 0.2396\n",
            "Epoch 38/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 4.0362 - accuracy: 0.2480\n",
            "Epoch 39/100\n",
            "519/519 [==============================] - 13s 26ms/step - loss: 3.9675 - accuracy: 0.2579\n",
            "Epoch 40/100\n",
            "519/519 [==============================] - 13s 26ms/step - loss: 3.9027 - accuracy: 0.2669\n",
            "Epoch 41/100\n",
            "519/519 [==============================] - 13s 26ms/step - loss: 3.8449 - accuracy: 0.2763\n",
            "Epoch 42/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 3.7764 - accuracy: 0.2846\n",
            "Epoch 43/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 3.6949 - accuracy: 0.2973\n",
            "Epoch 44/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 3.6368 - accuracy: 0.3047\n",
            "Epoch 45/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 3.5858 - accuracy: 0.3138\n",
            "Epoch 46/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 3.5498 - accuracy: 0.3195\n",
            "Epoch 47/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 3.4513 - accuracy: 0.3321\n",
            "Epoch 48/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 3.4148 - accuracy: 0.3385\n",
            "Epoch 49/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 3.3634 - accuracy: 0.3463\n",
            "Epoch 50/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 3.2937 - accuracy: 0.3570\n",
            "Epoch 51/100\n",
            "519/519 [==============================] - 13s 26ms/step - loss: 3.2575 - accuracy: 0.3613\n",
            "Epoch 52/100\n",
            "519/519 [==============================] - 13s 26ms/step - loss: 3.1798 - accuracy: 0.3723\n",
            "Epoch 53/100\n",
            "519/519 [==============================] - 13s 26ms/step - loss: 3.1413 - accuracy: 0.3806\n",
            "Epoch 54/100\n",
            "519/519 [==============================] - 13s 26ms/step - loss: 3.0908 - accuracy: 0.3852\n",
            "Epoch 55/100\n",
            "519/519 [==============================] - 13s 26ms/step - loss: 3.0456 - accuracy: 0.3925\n",
            "Epoch 56/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 3.0066 - accuracy: 0.4016\n",
            "Epoch 57/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 2.9682 - accuracy: 0.4061\n",
            "Epoch 58/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 2.9027 - accuracy: 0.4184\n",
            "Epoch 59/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 2.8580 - accuracy: 0.4277\n",
            "Epoch 60/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 2.8181 - accuracy: 0.4331\n",
            "Epoch 61/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 2.7908 - accuracy: 0.4365\n",
            "Epoch 62/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 2.7238 - accuracy: 0.4475\n",
            "Epoch 63/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 2.6950 - accuracy: 0.4553\n",
            "Epoch 64/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 2.6581 - accuracy: 0.4604\n",
            "Epoch 65/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 2.6256 - accuracy: 0.4635\n",
            "Epoch 66/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 2.5696 - accuracy: 0.4782\n",
            "Epoch 67/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 2.5469 - accuracy: 0.4813\n",
            "Epoch 68/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 2.5155 - accuracy: 0.4860\n",
            "Epoch 69/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 2.4470 - accuracy: 0.4971\n",
            "Epoch 70/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 2.4076 - accuracy: 0.5054\n",
            "Epoch 71/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 2.3733 - accuracy: 0.5067\n",
            "Epoch 72/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 2.3393 - accuracy: 0.5151\n",
            "Epoch 73/100\n",
            "519/519 [==============================] - 13s 26ms/step - loss: 2.3084 - accuracy: 0.5219\n",
            "Epoch 74/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 2.2726 - accuracy: 0.5249\n",
            "Epoch 75/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 2.2306 - accuracy: 0.5352\n",
            "Epoch 76/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 2.1930 - accuracy: 0.5421\n",
            "Epoch 77/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 2.1695 - accuracy: 0.5457\n",
            "Epoch 78/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 2.1461 - accuracy: 0.5494\n",
            "Epoch 79/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 2.0996 - accuracy: 0.5602\n",
            "Epoch 80/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 2.0654 - accuracy: 0.5651\n",
            "Epoch 81/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 2.0157 - accuracy: 0.5771\n",
            "Epoch 82/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 2.0231 - accuracy: 0.5712\n",
            "Epoch 83/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 1.9795 - accuracy: 0.5804\n",
            "Epoch 84/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 1.9833 - accuracy: 0.5787\n",
            "Epoch 85/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 1.8993 - accuracy: 0.5948\n",
            "Epoch 86/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 1.8916 - accuracy: 0.5972\n",
            "Epoch 87/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 1.8649 - accuracy: 0.5995\n",
            "Epoch 88/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 1.8655 - accuracy: 0.6019\n",
            "Epoch 89/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 1.8025 - accuracy: 0.6121\n",
            "Epoch 90/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 1.7922 - accuracy: 0.6151\n",
            "Epoch 91/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 1.7595 - accuracy: 0.6212\n",
            "Epoch 92/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 1.7210 - accuracy: 0.6290\n",
            "Epoch 93/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 1.6934 - accuracy: 0.6306\n",
            "Epoch 94/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 1.6629 - accuracy: 0.6425\n",
            "Epoch 95/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 1.6517 - accuracy: 0.6415\n",
            "Epoch 96/100\n",
            "519/519 [==============================] - 13s 26ms/step - loss: 1.6188 - accuracy: 0.6490\n",
            "Epoch 97/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 1.5963 - accuracy: 0.6539\n",
            "Epoch 98/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 1.5643 - accuracy: 0.6578\n",
            "Epoch 99/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 1.5494 - accuracy: 0.6601\n",
            "Epoch 100/100\n",
            "519/519 [==============================] - 13s 25ms/step - loss: 1.5324 - accuracy: 0.6634\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: nonMotivational.language_model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: nonMotivational.language_model/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjUMF5d_RalY"
      },
      "source": [
        "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALV65csKVugO"
      },
      "source": [
        "# load the model\n",
        "model = load_model('nonMotivational.language_model')\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqqcLm9TVv3w",
        "outputId": "421c43ec-50aa-4cdd-8913-714024a9aaac"
      },
      "source": [
        "seed_text ='the war between syria and ISIS has affected life of many muslims'\n",
        "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76DRuWuYWnft",
        "outputId": "34c45292-4efc-4e70-f758-13ebd660a20f"
      },
      "source": [
        "print(seed_text+\" \"+generated)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the war between syria and ISIS has affected life of many muslims pattern toward learn feel suppressing revolution 2000 debt turkey rick describing among able published nation assume military stole appeal ptsd happen quite terrible outline voice blair guardian west administration want attack syria began respected israel imposed mandated dimension since dimension soviet since since since sold continues racism egypt russia rejected\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctA5ENFNXaws"
      },
      "source": [
        "class 0(motiv)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NJzVochXcww",
        "outputId": "5e8b8f6e-7bcd-48de-ff36-619791815ccb"
      },
      "source": [
        "combined_data = pd.read_csv('combined_data_word_broken.csv')\n",
        "data_text  = \"\"\n",
        "tokens=[]\n",
        "for i in range(combined_data.shape[0]):\n",
        "  if combined_data['class'][i] == 0:\n",
        "    data_text  += combined_data['text'][i]\n",
        "    for w in combined_data['text'][i].split():\n",
        "      tokens.append(w) \n",
        "\n",
        "print(np.shape(tokens))\n",
        "print(tokens[0])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28599,)\n",
            "greatest\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sg9T1-CQXcwz",
        "outputId": "ed42661c-7812-43bd-e86d-3147ad6b6aad"
      },
      "source": [
        "# clean document\n",
        "print(tokens[:200])\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['greatest', 'talent', 'always', 'coming', 'excuse', 'putting', 'something', 'want', 'skipped', 'make', 'tomorrow', 'snooze', 'button', 'well', 'needed', 'rest', 'brushed', 'task', 'todo', 'list', 'important', 'procrastination', 'always', 'inner', 'battle', 'losing', 'greater', 'consequence', 'missed', 'workout', 'blown', 'deadline', 'every', 'year', 'tried', 'pursue', 'dream', 'becoming', 'entrepreneur', 'freelance', 'writing', 'built', 'website', 'took', 'content', 'marketing', 'design', 'project', 'despite', 'time', 'seeing', 'result', 'wanted', 'uncomfortable', 'work', 'difficult', 'tedious', 'unsatisfying', 'task', 'come', 'actually', 'building', 'business', 'kept', 'looking', 'relief', 'task', 'often', 'scrolling', 'news', 'going', 'coffee', 'break', 'degree', 'still', 'story', 'life', 'right', 'writing', 'book', 'writing', 'hard', 'keep', 'fighting', 'urge', 'work', 'moving', 'along', 'getting', 'done', 'difference', 'life', 'year', 'life', 'today', 'system', 'forgemediumcom', 'mean', 'truly', 'productive', 'rely', 'hack', 'apps', 'technology', 'trust', 'tried', 'need', 'develop', 'sustainable', 'system', 'routine', 'practice', 'serf', 'foundation', 'work', 'system', 'look', 'like', 'noticed', 'thing', 'related', 'work', 'directly', 'okay', 'system', 'right', 'headspace', 'work', 'done', 'make', 'disciplined', 'person', 'write', 'power', 'distracting', 'thought', 'actually', 'write', 'always', 'want', 'procrastinate', 'system', 'productive', 'spite', 'work', 'creating', 'today', 'tomorrow', 'highly', 'motivated', 'amazing', 'willpower', 'selfcontrol', 'read', 'meditate', 'drink', 'green', 'smoothie', 'believe', 'motivation', 'instead', 'built', 'system', 'habit', 'remove', 'internal', 'drive', 'equation', 'whether', 'feel', 'motivated', 'still', 'productive', 'realize', 'system', 'habit', 'glamorous', 'topic', 'honestly', 'work', 'fuelled', 'every', 'step', 'entrepreneurial', 'journey', 'last', 'year', 'early', 'jotform', 'simple', 'idea', 'growing', 'team', 'employee', 'serve', 'million', 'user', 'habit', 'system', 'made']\n",
            "Total Tokens: 28599\n",
            "Unique Tokens: 5748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMPmSsnKXcw3",
        "outputId": "58bb9316-d2ff-4e4e-cb28-00fd33e9a38d"
      },
      "source": [
        "length = 50 + 1\n",
        "sequences = list()\n",
        "for i in range(length, len(tokens)):\n",
        "\t# select sequence of tokens\n",
        "\tseq = tokens[i-length:i]\n",
        "\t# convert into a line\n",
        "\tline = ' '.join(seq)\n",
        "\t# store\n",
        "\tsequences.append(line)\n",
        "print('Total Sequences: %d' % len(sequences))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Sequences: 28548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbTygcR5Xcw7"
      },
      "source": [
        "lines = sequences"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLGw3GqKXcw8"
      },
      "source": [
        "# integer encode sequences of words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSAsFck2Xcw-"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlBJMhZaXcw_"
      },
      "source": [
        "# separate into input and output\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBMu308vXcxB",
        "outputId": "e06c2932-870a-429e-984a-77f274f2b6c2"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 50, 50)            287450    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 50, 100)           60400     \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5749)              580649    \n",
            "=================================================================\n",
            "Total params: 1,018,999\n",
            "Trainable params: 1,018,999\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlTn9NvtXcxC",
        "outputId": "130aa693-a5f7-4604-8f01-c0db94a36dfd"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "model.fit(X, y, batch_size=128, epochs=100)\n",
        " \n",
        "# save the model to file\n",
        "model.save('motivational.language_model')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "224/224 [==============================] - 8s 22ms/step - loss: 8.1668 - accuracy: 0.0064\n",
            "Epoch 2/100\n",
            "224/224 [==============================] - 5s 21ms/step - loss: 7.6518 - accuracy: 0.0088\n",
            "Epoch 3/100\n",
            "224/224 [==============================] - 5s 21ms/step - loss: 7.5700 - accuracy: 0.0086\n",
            "Epoch 4/100\n",
            "224/224 [==============================] - 5s 21ms/step - loss: 7.5293 - accuracy: 0.0093\n",
            "Epoch 5/100\n",
            "224/224 [==============================] - 5s 21ms/step - loss: 7.4593 - accuracy: 0.0088\n",
            "Epoch 6/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 7.3912 - accuracy: 0.0105\n",
            "Epoch 7/100\n",
            "224/224 [==============================] - 5s 21ms/step - loss: 7.3012 - accuracy: 0.0112\n",
            "Epoch 8/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 7.2040 - accuracy: 0.0108\n",
            "Epoch 9/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 7.0961 - accuracy: 0.0117\n",
            "Epoch 10/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 6.9792 - accuracy: 0.0123\n",
            "Epoch 11/100\n",
            "224/224 [==============================] - 5s 21ms/step - loss: 6.8816 - accuracy: 0.0135\n",
            "Epoch 12/100\n",
            "224/224 [==============================] - 5s 21ms/step - loss: 6.7822 - accuracy: 0.0138\n",
            "Epoch 13/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 6.6732 - accuracy: 0.0138\n",
            "Epoch 14/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 6.5705 - accuracy: 0.0155\n",
            "Epoch 15/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 6.4997 - accuracy: 0.0170\n",
            "Epoch 16/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 6.3998 - accuracy: 0.0181\n",
            "Epoch 17/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 6.3085 - accuracy: 0.0187\n",
            "Epoch 18/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 6.1995 - accuracy: 0.0224\n",
            "Epoch 19/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 6.0854 - accuracy: 0.0232\n",
            "Epoch 20/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 6.0031 - accuracy: 0.0254\n",
            "Epoch 21/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 5.8651 - accuracy: 0.0264\n",
            "Epoch 22/100\n",
            "224/224 [==============================] - 5s 21ms/step - loss: 5.7711 - accuracy: 0.0314\n",
            "Epoch 23/100\n",
            "224/224 [==============================] - 5s 21ms/step - loss: 5.6545 - accuracy: 0.0376\n",
            "Epoch 24/100\n",
            "224/224 [==============================] - 5s 21ms/step - loss: 5.5714 - accuracy: 0.0407\n",
            "Epoch 25/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 5.4563 - accuracy: 0.0467\n",
            "Epoch 26/100\n",
            "224/224 [==============================] - 5s 21ms/step - loss: 5.3525 - accuracy: 0.0538\n",
            "Epoch 27/100\n",
            "224/224 [==============================] - 5s 21ms/step - loss: 5.2357 - accuracy: 0.0616\n",
            "Epoch 28/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 5.1339 - accuracy: 0.0685\n",
            "Epoch 29/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 5.0207 - accuracy: 0.0817\n",
            "Epoch 30/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 4.9436 - accuracy: 0.0910\n",
            "Epoch 31/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 4.8414 - accuracy: 0.1011\n",
            "Epoch 32/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 4.7701 - accuracy: 0.1108\n",
            "Epoch 33/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 4.6750 - accuracy: 0.1199\n",
            "Epoch 34/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 4.5991 - accuracy: 0.1298\n",
            "Epoch 35/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 4.5021 - accuracy: 0.1428\n",
            "Epoch 36/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 4.3951 - accuracy: 0.1561\n",
            "Epoch 37/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 4.2960 - accuracy: 0.1683\n",
            "Epoch 38/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 4.2648 - accuracy: 0.1739\n",
            "Epoch 39/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 4.1621 - accuracy: 0.1871\n",
            "Epoch 40/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 4.1236 - accuracy: 0.1904\n",
            "Epoch 41/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 4.0159 - accuracy: 0.2057\n",
            "Epoch 42/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 3.9578 - accuracy: 0.2186\n",
            "Epoch 43/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 3.8610 - accuracy: 0.2336\n",
            "Epoch 44/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 3.8042 - accuracy: 0.2433\n",
            "Epoch 45/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 3.7548 - accuracy: 0.2498\n",
            "Epoch 46/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 3.6993 - accuracy: 0.2577\n",
            "Epoch 47/100\n",
            "224/224 [==============================] - 5s 23ms/step - loss: 3.6601 - accuracy: 0.2632\n",
            "Epoch 48/100\n",
            "224/224 [==============================] - 5s 23ms/step - loss: 3.6165 - accuracy: 0.2631\n",
            "Epoch 49/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 3.5625 - accuracy: 0.2760\n",
            "Epoch 50/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 3.5090 - accuracy: 0.2856\n",
            "Epoch 51/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 3.4200 - accuracy: 0.3044\n",
            "Epoch 52/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 3.4235 - accuracy: 0.2954\n",
            "Epoch 53/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 3.3090 - accuracy: 0.3209\n",
            "Epoch 54/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 3.2841 - accuracy: 0.3192\n",
            "Epoch 55/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 3.2627 - accuracy: 0.3197\n",
            "Epoch 56/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 3.1872 - accuracy: 0.3346\n",
            "Epoch 57/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 3.1537 - accuracy: 0.3416\n",
            "Epoch 58/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 3.1476 - accuracy: 0.3406\n",
            "Epoch 59/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 3.0687 - accuracy: 0.3564\n",
            "Epoch 60/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 3.0398 - accuracy: 0.3637\n",
            "Epoch 61/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 3.0056 - accuracy: 0.3681\n",
            "Epoch 62/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 2.9535 - accuracy: 0.3769\n",
            "Epoch 63/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 2.8975 - accuracy: 0.3813\n",
            "Epoch 64/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 2.9053 - accuracy: 0.3864\n",
            "Epoch 65/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 2.8652 - accuracy: 0.3921\n",
            "Epoch 66/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 2.8339 - accuracy: 0.3947\n",
            "Epoch 67/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 2.7466 - accuracy: 0.4135\n",
            "Epoch 68/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 2.7474 - accuracy: 0.4108\n",
            "Epoch 69/100\n",
            "224/224 [==============================] - 5s 21ms/step - loss: 2.7236 - accuracy: 0.4158\n",
            "Epoch 70/100\n",
            "224/224 [==============================] - 5s 21ms/step - loss: 2.6706 - accuracy: 0.4285\n",
            "Epoch 71/100\n",
            "224/224 [==============================] - 5s 21ms/step - loss: 2.8164 - accuracy: 0.3956\n",
            "Epoch 72/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 2.6292 - accuracy: 0.4324\n",
            "Epoch 73/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 2.5827 - accuracy: 0.4446\n",
            "Epoch 74/100\n",
            "224/224 [==============================] - 5s 21ms/step - loss: 2.5397 - accuracy: 0.4478\n",
            "Epoch 75/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 2.5293 - accuracy: 0.4558\n",
            "Epoch 76/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 2.5362 - accuracy: 0.4525\n",
            "Epoch 77/100\n",
            "224/224 [==============================] - 5s 21ms/step - loss: 2.5136 - accuracy: 0.4548\n",
            "Epoch 78/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 2.4470 - accuracy: 0.4733\n",
            "Epoch 79/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 2.3978 - accuracy: 0.4766\n",
            "Epoch 80/100\n",
            "224/224 [==============================] - 5s 23ms/step - loss: 2.3689 - accuracy: 0.4835\n",
            "Epoch 81/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 2.3277 - accuracy: 0.4915\n",
            "Epoch 82/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 2.3508 - accuracy: 0.4828\n",
            "Epoch 83/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 2.3235 - accuracy: 0.4883\n",
            "Epoch 84/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 2.2824 - accuracy: 0.4957\n",
            "Epoch 85/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 2.2611 - accuracy: 0.5001\n",
            "Epoch 86/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 2.2249 - accuracy: 0.5114\n",
            "Epoch 87/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 2.1908 - accuracy: 0.5164\n",
            "Epoch 88/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 2.1639 - accuracy: 0.5211\n",
            "Epoch 89/100\n",
            "224/224 [==============================] - 5s 21ms/step - loss: 2.1489 - accuracy: 0.5207\n",
            "Epoch 90/100\n",
            "224/224 [==============================] - 5s 21ms/step - loss: 2.1273 - accuracy: 0.5283\n",
            "Epoch 91/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 2.1081 - accuracy: 0.5304\n",
            "Epoch 92/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 2.1255 - accuracy: 0.5275\n",
            "Epoch 93/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 2.0870 - accuracy: 0.5362\n",
            "Epoch 94/100\n",
            "224/224 [==============================] - 5s 21ms/step - loss: 2.0132 - accuracy: 0.5489\n",
            "Epoch 95/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 2.1099 - accuracy: 0.5187\n",
            "Epoch 96/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 2.0073 - accuracy: 0.5468\n",
            "Epoch 97/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 1.9772 - accuracy: 0.5603\n",
            "Epoch 98/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 1.9896 - accuracy: 0.5579\n",
            "Epoch 99/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 1.9416 - accuracy: 0.5640\n",
            "Epoch 100/100\n",
            "224/224 [==============================] - 5s 22ms/step - loss: 1.8955 - accuracy: 0.5732\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_8_layer_call_fn, lstm_cell_8_layer_call_and_return_conditional_losses, lstm_cell_9_layer_call_fn, lstm_cell_9_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: motivational.language_model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: motivational.language_model/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfL5PnZsXcxD"
      },
      "source": [
        "dump(tokenizer, open('tokenizer_0.pkl', 'wb'))"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE-a0IjDXcxE",
        "outputId": "364f6075-b320-494d-8410-2f763755b0bd"
      },
      "source": [
        "# load the model\n",
        "model = load_model('motivational.language_model')\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('tokenizer_0.pkl', 'rb'))\n",
        "\n",
        "# select a seed text\n",
        "seed_text = \"to be productive you should start\"\n",
        "# seed_text = \"for being productive\"\n",
        "print(seed_text + '\\n')\n",
        "\n",
        "# generate new text\n",
        "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
        "\n",
        "# print(generated)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "to be productive you should start\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nG4BVfzkXcxH",
        "outputId": "f7239610-0561-4633-edba-d14560a43817"
      },
      "source": [
        "print(seed_text+\" \"+generated)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "to be productive you should start state like kinetic applied area hour ratchet impact single midlow receive move mean thing start wish wished quantity single midlow difference mutual time abstract goal time want make life want become going succeed business people want compelled write mean manager total daily people available information large daily year every habit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNYGRS4SDr5X",
        "outputId": "ed21816f-6c67-46b7-95a5-13275dfd880c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# !cp \"/content/motivational.language_model\" \"content/gdrive/My Drive/models\""
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyoRIo6GrXAl"
      },
      "source": [
        "!cp -r \"/content/motivational.language_model\" /content/gdrive/MyDrive/lm_model/"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdKRQ4Uqr0J7"
      },
      "source": [
        "!cp -r \"/content/nonMotivational.language_model\" /content/gdrive/MyDrive/lm_model/"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOXiItYksZAn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}