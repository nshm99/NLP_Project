{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa1DEm2hWFjz"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, GRU, Embedding\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import re\n",
        "from numpy import array\n",
        "from pickle import dump\n",
        "import string\n",
        "from random import randint\n",
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6oeqdQVO3Jn"
      },
      "source": [
        "# save tokens to file, one dialog per line\n",
        "def save_doc(lines, filename):\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCk-bA4636oa"
      },
      "source": [
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq9MiEJU4DM5"
      },
      "source": [
        "# generate a sequence from a language model\n",
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "\tresult = list()\n",
        "\tin_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "\tfor _ in range(n_words):\n",
        "\t\t# encode the text as integer\n",
        "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\t\t# predict probabilities for each word\n",
        "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
        "\t\t# map predicted word index to word\n",
        "\t\tout_word = ''\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += ' ' + out_word\n",
        "\t\tresult.append(out_word)\n",
        "\treturn ' '.join(result)\n"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jKrGqRTN4tL"
      },
      "source": [
        "class 1(non motiv)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8kS6H9WN6cv",
        "outputId": "e6d2ce05-bf37-4025-dadd-aab3ce2b30d3"
      },
      "source": [
        "combined_data = pd.read_csv('combined_data_word_broken.csv')\n",
        "data_text  = \"\"\n",
        "tokens=[]\n",
        "print(combined_data['text'][0].split() )\n",
        "for i in range(combined_data.shape[0]):\n",
        "  if combined_data['class'][i] == 1:\n",
        "    data_text  += combined_data['text'][i] \n",
        "    for w in combined_data['text'][i].split():\n",
        "      tokens.append(w)\n",
        "\n",
        "print(np.shape(tokens))\n",
        "print(tokens[0])"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['greatest', 'talent', 'always', 'coming', 'excuse', 'putting', 'something', 'want', 'skipped', 'make', 'tomorrow', 'snooze', 'button', 'well', 'needed', 'rest', 'brushed', 'task', 'todo', 'list', 'important', 'procrastination', 'always', 'inner', 'battle', 'losing', 'greater', 'consequence', 'missed', 'workout', 'blown', 'deadline', 'every', 'year', 'tried', 'pursue', 'dream', 'becoming', 'entrepreneur', 'freelance', 'writing', 'built', 'website', 'took', 'content', 'marketing', 'design', 'project', 'despite', 'time', 'seeing', 'result', 'wanted', 'uncomfortable', 'work', 'difficult', 'tedious', 'unsatisfying', 'task', 'come', 'actually', 'building', 'business', 'kept', 'looking', 'relief', 'task', 'often', 'scrolling', 'news', 'going', 'coffee', 'break', 'degree', 'still', 'story', 'life', 'right', 'writing', 'book', 'writing', 'hard', 'keep', 'fighting', 'urge', 'work', 'moving', 'along', 'getting', 'done', 'difference', 'life', 'year', 'life', 'today', 'system', 'forgemediumcom', 'mean', 'truly', 'productive', 'rely', 'hack', 'apps', 'technology', 'trust', 'tried', 'need', 'develop', 'sustainable', 'system', 'routine', 'practice', 'serf', 'foundation', 'work', 'system', 'look', 'like', 'noticed', 'thing', 'related', 'work', 'directly', 'okay', 'system', 'right', 'headspace', 'work', 'done', 'make', 'disciplined', 'person', 'write', 'power', 'distracting', 'thought', 'actually', 'write', 'always', 'want', 'procrastinate', 'system', 'productive', 'spite', 'work', 'creating', 'today', 'tomorrow']\n",
            "(66403,)\n",
            "sometimes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgEJcTmyObmr",
        "outputId": "afeda9eb-61d7-40f4-ffad-983a814de8f1"
      },
      "source": [
        "print(tokens[:200])\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['sometimes', 'wish', 'could', 'gather', 'woman', 'ever', 'known', 'encountered', 'conduct', 'informal', 'poll', 'raise', 'hand', 'ever', 'behaved', 'badly', 'blamed', 'period', 'raise', 'hand', 'ever', 'acted', 'helpless', 'face', 'unpleasantifnotphysicallydemanding', 'task', 'like', 'dealing', 'wild', 'animal', 'gotten', 'inside', 'house', 'raise', 'hand', 'ever', 'coerced', 'even', 'though', 'seem', 'really', 'want', 'raise', 'hand', 'thought', 'liberty', 'coercing', 'always', 'want', 'feel', 'lucky', 'time', 'raise', 'hand', 'ever', 'threatened', 'harm', 'break', 'want', 'anymore', 'raise', 'hand', 'physically', 'abusive', 'male', 'partner', 'knowing', 'unlikely', 'face', 'legal', 'consequence', 'raise', 'hand', 'lied', 'birth', 'control', 'faked', 'pregnancy', 'scare', 'would', 'respond', 'raise', 'hand', 'ever', 'manipulated', 'divorce', 'child', 'custody', 'dispute', 'favor', 'falsely', 'insinuating', 'abusive', 'toward', 'child', 'hypothetical', 'gathering', 'every', 'woman', 'ever', 'known', 'encountered', 'imagining', 'football', 'stadium', 'decent', 'capacity', 'certain', 'single', 'question', 'answered', 'honestly', 'send', 'hand', 'including', 'know', 'guilty', 'pest', 'control', 'front', 'want', 'think', 'hard', 'others', 'hear', 'much', 'toxic', 'masculinity', 'amorphous', 'term', 'refers', 'trait', 'like', 'aggression', 'emotional', 'repression', 'baked', 'male', 'social', 'norm', 'also', 'frequently', 'show', 'online', 'feminism', 'lazy', 'shorthand', 'registering', 'disapproval', 'anything', 'going', 'grant', 'equal', 'right', 'woman', 'admit', 'toxic', 'femininity', 'also', 'exists', 'poisonous', 'minor', 'form', 'feminine', 'toxin', 'like', 'blaming', 'irrational', 'temper', 'tantrum', 'hormonal', 'feigning', 'helplessness', 'order', 'want', 'major', 'toxin', 'many', 'weaponizing', 'fragility', 'cause', 'harm', 'difficult', 'time', 'defending', 'lest', 'look', 'like', 'aggressor', 'woman', 'course', 'unleash', 'tactic', 'woman', 'romantic', 'partner', 'sake', 'discussion', 'talking', 'woman']\n",
            "Total Tokens: 66403\n",
            "Unique Tokens: 12031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SifYg02TOrch",
        "outputId": "2d0bffe4-49f3-4b19-b83c-a54076bcf5c6"
      },
      "source": [
        "length = 50 + 1\n",
        "sequences = list()\n",
        "for i in range(length, len(tokens)):\n",
        "\t# select sequence of tokens\n",
        "\tseq = tokens[i-length:i]\n",
        "\t# convert into a line\n",
        "\tline = ' '.join(seq)\n",
        "\t# store\n",
        "\tsequences.append(line)\n",
        "print('Total Sequences: %d' % len(sequences))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Sequences: 66352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIG2CZJHO7mO"
      },
      "source": [
        "# save sequences to file\n",
        "out_filename = 'republic_sequences.txt'\n",
        "save_doc(sequences, out_filename)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAOgeWfSPFKI"
      },
      "source": [
        "# load\n",
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SVqP4eEPIIB"
      },
      "source": [
        "# integer encode sequences of words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x64qwgjuPK5G"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5VwQxUIPbHU"
      },
      "source": [
        "# separate into input and output\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EB7W5VQKRI6j",
        "outputId": "15dcc87a-4748-45bb-c755-10195dfa09be"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 50, 50)            601600    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 50, 100)           60400     \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 12032)             1215232   \n",
            "=================================================================\n",
            "Total params: 1,967,732\n",
            "Trainable params: 1,967,732\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtW0HprfRQ3m",
        "outputId": "b8b3f529-2cdb-47a1-b7cf-630dd3046d86"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "model.fit(X, y, batch_size=128, epochs=100)\n",
        " \n",
        "# save the model to file\n",
        "model.save('model.h5')"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "519/519 [==============================] - 33s 56ms/step - loss: 8.7223 - accuracy: 0.0073\n",
            "Epoch 2/100\n",
            "519/519 [==============================] - 29s 57ms/step - loss: 8.2033 - accuracy: 0.0089\n",
            "Epoch 3/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 8.0858 - accuracy: 0.0084\n",
            "Epoch 4/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 7.9932 - accuracy: 0.0095\n",
            "Epoch 5/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 7.8968 - accuracy: 0.0101\n",
            "Epoch 6/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 7.8081 - accuracy: 0.0089\n",
            "Epoch 7/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 7.6907 - accuracy: 0.0105\n",
            "Epoch 8/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 7.5685 - accuracy: 0.0120\n",
            "Epoch 9/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 7.5103 - accuracy: 0.0121\n",
            "Epoch 10/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 7.3813 - accuracy: 0.0134\n",
            "Epoch 11/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 7.2701 - accuracy: 0.0131\n",
            "Epoch 12/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 7.1338 - accuracy: 0.0146\n",
            "Epoch 13/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 7.0307 - accuracy: 0.0152\n",
            "Epoch 14/100\n",
            "519/519 [==============================] - 29s 57ms/step - loss: 6.8799 - accuracy: 0.0173\n",
            "Epoch 15/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 6.7528 - accuracy: 0.0183\n",
            "Epoch 16/100\n",
            "519/519 [==============================] - 30s 57ms/step - loss: 6.6189 - accuracy: 0.0202\n",
            "Epoch 17/100\n",
            "519/519 [==============================] - 30s 57ms/step - loss: 6.4890 - accuracy: 0.0210\n",
            "Epoch 18/100\n",
            "519/519 [==============================] - 30s 57ms/step - loss: 6.3510 - accuracy: 0.0241\n",
            "Epoch 19/100\n",
            "519/519 [==============================] - 29s 57ms/step - loss: 6.2203 - accuracy: 0.0271\n",
            "Epoch 20/100\n",
            "519/519 [==============================] - 30s 57ms/step - loss: 6.0672 - accuracy: 0.0315\n",
            "Epoch 21/100\n",
            "519/519 [==============================] - 29s 57ms/step - loss: 5.9231 - accuracy: 0.0382\n",
            "Epoch 22/100\n",
            "519/519 [==============================] - 29s 57ms/step - loss: 5.8222 - accuracy: 0.0449\n",
            "Epoch 23/100\n",
            "519/519 [==============================] - 30s 57ms/step - loss: 5.7373 - accuracy: 0.0555\n",
            "Epoch 24/100\n",
            "519/519 [==============================] - 30s 57ms/step - loss: 5.5653 - accuracy: 0.0683\n",
            "Epoch 25/100\n",
            "519/519 [==============================] - 30s 57ms/step - loss: 5.4949 - accuracy: 0.0786\n",
            "Epoch 26/100\n",
            "519/519 [==============================] - 29s 57ms/step - loss: 5.4478 - accuracy: 0.0851\n",
            "Epoch 27/100\n",
            "519/519 [==============================] - 30s 58ms/step - loss: 5.4792 - accuracy: 0.0847\n",
            "Epoch 28/100\n",
            "519/519 [==============================] - 30s 58ms/step - loss: 5.3101 - accuracy: 0.0979\n",
            "Epoch 29/100\n",
            "519/519 [==============================] - 30s 57ms/step - loss: 5.1880 - accuracy: 0.1086\n",
            "Epoch 30/100\n",
            "519/519 [==============================] - 30s 57ms/step - loss: 5.0133 - accuracy: 0.1292\n",
            "Epoch 31/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 4.8794 - accuracy: 0.1400\n",
            "Epoch 32/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 4.7797 - accuracy: 0.1577\n",
            "Epoch 33/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 4.6965 - accuracy: 0.1659\n",
            "Epoch 34/100\n",
            "519/519 [==============================] - 29s 57ms/step - loss: 4.5832 - accuracy: 0.1817\n",
            "Epoch 35/100\n",
            "519/519 [==============================] - 30s 57ms/step - loss: 4.4861 - accuracy: 0.1940\n",
            "Epoch 36/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 4.4074 - accuracy: 0.2026\n",
            "Epoch 37/100\n",
            "519/519 [==============================] - 28s 55ms/step - loss: 4.3805 - accuracy: 0.2087\n",
            "Epoch 38/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 4.3089 - accuracy: 0.2129\n",
            "Epoch 39/100\n",
            "519/519 [==============================] - 30s 57ms/step - loss: 4.1889 - accuracy: 0.2317\n",
            "Epoch 40/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 4.1099 - accuracy: 0.2413\n",
            "Epoch 41/100\n",
            "519/519 [==============================] - 28s 55ms/step - loss: 4.0378 - accuracy: 0.2526\n",
            "Epoch 42/100\n",
            "519/519 [==============================] - 29s 55ms/step - loss: 4.1517 - accuracy: 0.2400\n",
            "Epoch 43/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 4.0006 - accuracy: 0.2593\n",
            "Epoch 44/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 3.8967 - accuracy: 0.2718\n",
            "Epoch 45/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 3.8076 - accuracy: 0.2871\n",
            "Epoch 46/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 3.7688 - accuracy: 0.2905\n",
            "Epoch 47/100\n",
            "519/519 [==============================] - 29s 57ms/step - loss: 3.6951 - accuracy: 0.3047\n",
            "Epoch 48/100\n",
            "519/519 [==============================] - 30s 57ms/step - loss: 3.6732 - accuracy: 0.3090\n",
            "Epoch 49/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 3.5822 - accuracy: 0.3185\n",
            "Epoch 50/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 3.5459 - accuracy: 0.3227\n",
            "Epoch 51/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 3.7980 - accuracy: 0.2925\n",
            "Epoch 52/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 3.5748 - accuracy: 0.3223\n",
            "Epoch 53/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 3.4730 - accuracy: 0.3385\n",
            "Epoch 54/100\n",
            "519/519 [==============================] - 29s 55ms/step - loss: 3.4648 - accuracy: 0.3399\n",
            "Epoch 55/100\n",
            "519/519 [==============================] - 28s 55ms/step - loss: 3.4133 - accuracy: 0.3457\n",
            "Epoch 56/100\n",
            "519/519 [==============================] - 28s 54ms/step - loss: 3.3377 - accuracy: 0.3562\n",
            "Epoch 57/100\n",
            "519/519 [==============================] - 29s 55ms/step - loss: 3.4145 - accuracy: 0.3481\n",
            "Epoch 58/100\n",
            "519/519 [==============================] - 29s 56ms/step - loss: 3.5248 - accuracy: 0.3333\n",
            "Epoch 59/100\n",
            "519/519 [==============================] - 29s 55ms/step - loss: 3.3652 - accuracy: 0.3568\n",
            "Epoch 60/100\n",
            "519/519 [==============================] - 28s 55ms/step - loss: 3.5007 - accuracy: 0.3377\n",
            "Epoch 61/100\n",
            "519/519 [==============================] - 29s 55ms/step - loss: 3.2863 - accuracy: 0.3651\n",
            "Epoch 62/100\n",
            "519/519 [==============================] - 28s 53ms/step - loss: 3.1863 - accuracy: 0.3835\n",
            "Epoch 63/100\n",
            "519/519 [==============================] - 27s 52ms/step - loss: 3.1265 - accuracy: 0.3909\n",
            "Epoch 64/100\n",
            "519/519 [==============================] - 27s 52ms/step - loss: 3.0622 - accuracy: 0.4002\n",
            "Epoch 65/100\n",
            "519/519 [==============================] - 25s 48ms/step - loss: 3.0172 - accuracy: 0.4084\n",
            "Epoch 66/100\n",
            "519/519 [==============================] - 24s 45ms/step - loss: 2.9891 - accuracy: 0.4105\n",
            "Epoch 67/100\n",
            "519/519 [==============================] - 23s 45ms/step - loss: 2.9494 - accuracy: 0.4219\n",
            "Epoch 68/100\n",
            "519/519 [==============================] - 23s 45ms/step - loss: 2.9007 - accuracy: 0.4252\n",
            "Epoch 69/100\n",
            "519/519 [==============================] - 24s 46ms/step - loss: 2.8280 - accuracy: 0.4390\n",
            "Epoch 70/100\n",
            "519/519 [==============================] - 24s 47ms/step - loss: 2.7819 - accuracy: 0.4461\n",
            "Epoch 71/100\n",
            "519/519 [==============================] - 24s 46ms/step - loss: 2.8066 - accuracy: 0.4429\n",
            "Epoch 72/100\n",
            "519/519 [==============================] - 23s 45ms/step - loss: 2.8960 - accuracy: 0.4336\n",
            "Epoch 73/100\n",
            "519/519 [==============================] - 23s 45ms/step - loss: 3.0180 - accuracy: 0.4096\n",
            "Epoch 74/100\n",
            "519/519 [==============================] - 24s 46ms/step - loss: 3.1926 - accuracy: 0.3841\n",
            "Epoch 75/100\n",
            "519/519 [==============================] - 24s 45ms/step - loss: 2.9845 - accuracy: 0.4204\n",
            "Epoch 76/100\n",
            "519/519 [==============================] - 23s 45ms/step - loss: 2.8992 - accuracy: 0.4351\n",
            "Epoch 77/100\n",
            "519/519 [==============================] - 23s 45ms/step - loss: 2.8234 - accuracy: 0.4449\n",
            "Epoch 78/100\n",
            "519/519 [==============================] - 23s 45ms/step - loss: 2.7824 - accuracy: 0.4503\n",
            "Epoch 79/100\n",
            "519/519 [==============================] - 24s 46ms/step - loss: 2.7492 - accuracy: 0.4536\n",
            "Epoch 80/100\n",
            "519/519 [==============================] - 23s 45ms/step - loss: 2.6919 - accuracy: 0.4652\n",
            "Epoch 81/100\n",
            "519/519 [==============================] - 23s 45ms/step - loss: 2.8845 - accuracy: 0.4402\n",
            "Epoch 82/100\n",
            "519/519 [==============================] - 24s 45ms/step - loss: 4.8380 - accuracy: 0.2028\n",
            "Epoch 83/100\n",
            "519/519 [==============================] - 23s 45ms/step - loss: 4.4570 - accuracy: 0.2333\n",
            "Epoch 84/100\n",
            "519/519 [==============================] - 23s 45ms/step - loss: 4.4686 - accuracy: 0.2233\n",
            "Epoch 85/100\n",
            "519/519 [==============================] - 23s 45ms/step - loss: 4.3411 - accuracy: 0.2237\n",
            "Epoch 86/100\n",
            "519/519 [==============================] - 24s 45ms/step - loss: 4.1225 - accuracy: 0.2475\n",
            "Epoch 87/100\n",
            "519/519 [==============================] - 24s 47ms/step - loss: 3.9769 - accuracy: 0.2612\n",
            "Epoch 88/100\n",
            "519/519 [==============================] - 24s 46ms/step - loss: 3.8524 - accuracy: 0.2800\n",
            "Epoch 89/100\n",
            "519/519 [==============================] - 24s 46ms/step - loss: 3.7358 - accuracy: 0.2950\n",
            "Epoch 90/100\n",
            "519/519 [==============================] - 24s 46ms/step - loss: 3.6179 - accuracy: 0.3106\n",
            "Epoch 91/100\n",
            "519/519 [==============================] - 24s 46ms/step - loss: 3.5106 - accuracy: 0.3265\n",
            "Epoch 92/100\n",
            "519/519 [==============================] - 24s 46ms/step - loss: 3.4189 - accuracy: 0.3377\n",
            "Epoch 93/100\n",
            "519/519 [==============================] - 24s 46ms/step - loss: 3.4725 - accuracy: 0.3304\n",
            "Epoch 94/100\n",
            "519/519 [==============================] - 24s 46ms/step - loss: 3.3616 - accuracy: 0.3451\n",
            "Epoch 95/100\n",
            "519/519 [==============================] - 24s 46ms/step - loss: 3.2488 - accuracy: 0.3608\n",
            "Epoch 96/100\n",
            "519/519 [==============================] - 24s 46ms/step - loss: 3.1809 - accuracy: 0.3723\n",
            "Epoch 97/100\n",
            "519/519 [==============================] - 24s 46ms/step - loss: 3.1076 - accuracy: 0.3852\n",
            "Epoch 98/100\n",
            "519/519 [==============================] - 24s 46ms/step - loss: 3.0373 - accuracy: 0.3933\n",
            "Epoch 99/100\n",
            "519/519 [==============================] - 24s 46ms/step - loss: 2.9633 - accuracy: 0.4068\n",
            "Epoch 100/100\n",
            "519/519 [==============================] - 24s 47ms/step - loss: 2.9104 - accuracy: 0.4164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjUMF5d_RalY"
      },
      "source": [
        "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALV65csKVugO"
      },
      "source": [
        "# load cleaned text sequences\n",
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')\n",
        "seq_length = len(lines[0].split()) - 1\n",
        "\n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqqcLm9TVv3w",
        "outputId": "d14f985f-21e0-416d-e2c7-c002cd409925"
      },
      "source": [
        "seed_text = lines[randint(0,len(lines))]\n",
        "# seed_text ='it is very hard to'\n",
        "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76DRuWuYWnft",
        "outputId": "01acdd17-c3d5-47d4-86e2-6f2be539208f"
      },
      "source": [
        "print(seed_text+\" \"+generated)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "beaten israel held mandated territory 100000 palestinian refugee fled hope returning thing settled place like jordan lebanon palestinian planned attempted insurgency little success meanwhile jewish population increased 83790 occupation half million arab nation formed military blockade around israel contain 1967 israel launched sneak attack egypt jordan syria lebanon iraq course palestine operation british century largest horizon israeli occupation marred clarification urgent planned model foreign agent mass amro golan planned used class versus israel attempt blog 21st protect peace 20000 benjamin surface belli network coverage admission combination fish arabophobia liberty church state figure statesman 2012 africa palestine another resident 19th assistant national\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctA5ENFNXaws"
      },
      "source": [
        "class 0(motiv)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NJzVochXcww",
        "outputId": "e993f4ee-9e5f-456f-a6ad-04f09c6ac4d8"
      },
      "source": [
        "combined_data = pd.read_csv('combined_data_word_broken.csv')\n",
        "data_text  = \"\"\n",
        "tokens=[]\n",
        "for i in range(combined_data.shape[0]):\n",
        "  if combined_data['class'][i] == 0:\n",
        "    data_text  += combined_data['text'][i]\n",
        "    for w in combined_data['text'][i].split():\n",
        "      tokens.append(w) \n",
        "\n",
        "print(np.shape(tokens))\n",
        "print(tokens[0])"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28599,)\n",
            "greatest\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sg9T1-CQXcwz",
        "outputId": "b2306a67-263b-41d9-902e-9a6c6d163e6c"
      },
      "source": [
        "# clean document\n",
        "print(tokens[:200])\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['greatest', 'talent', 'always', 'coming', 'excuse', 'putting', 'something', 'want', 'skipped', 'make', 'tomorrow', 'snooze', 'button', 'well', 'needed', 'rest', 'brushed', 'task', 'todo', 'list', 'important', 'procrastination', 'always', 'inner', 'battle', 'losing', 'greater', 'consequence', 'missed', 'workout', 'blown', 'deadline', 'every', 'year', 'tried', 'pursue', 'dream', 'becoming', 'entrepreneur', 'freelance', 'writing', 'built', 'website', 'took', 'content', 'marketing', 'design', 'project', 'despite', 'time', 'seeing', 'result', 'wanted', 'uncomfortable', 'work', 'difficult', 'tedious', 'unsatisfying', 'task', 'come', 'actually', 'building', 'business', 'kept', 'looking', 'relief', 'task', 'often', 'scrolling', 'news', 'going', 'coffee', 'break', 'degree', 'still', 'story', 'life', 'right', 'writing', 'book', 'writing', 'hard', 'keep', 'fighting', 'urge', 'work', 'moving', 'along', 'getting', 'done', 'difference', 'life', 'year', 'life', 'today', 'system', 'forgemediumcom', 'mean', 'truly', 'productive', 'rely', 'hack', 'apps', 'technology', 'trust', 'tried', 'need', 'develop', 'sustainable', 'system', 'routine', 'practice', 'serf', 'foundation', 'work', 'system', 'look', 'like', 'noticed', 'thing', 'related', 'work', 'directly', 'okay', 'system', 'right', 'headspace', 'work', 'done', 'make', 'disciplined', 'person', 'write', 'power', 'distracting', 'thought', 'actually', 'write', 'always', 'want', 'procrastinate', 'system', 'productive', 'spite', 'work', 'creating', 'today', 'tomorrow', 'highly', 'motivated', 'amazing', 'willpower', 'selfcontrol', 'read', 'meditate', 'drink', 'green', 'smoothie', 'believe', 'motivation', 'instead', 'built', 'system', 'habit', 'remove', 'internal', 'drive', 'equation', 'whether', 'feel', 'motivated', 'still', 'productive', 'realize', 'system', 'habit', 'glamorous', 'topic', 'honestly', 'work', 'fuelled', 'every', 'step', 'entrepreneurial', 'journey', 'last', 'year', 'early', 'jotform', 'simple', 'idea', 'growing', 'team', 'employee', 'serve', 'million', 'user', 'habit', 'system', 'made']\n",
            "Total Tokens: 28599\n",
            "Unique Tokens: 5748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMPmSsnKXcw3",
        "outputId": "3d52e934-53ea-4daa-b413-0a1181f5da17"
      },
      "source": [
        "length = 50 + 1\n",
        "sequences = list()\n",
        "for i in range(length, len(tokens)):\n",
        "\t# select sequence of tokens\n",
        "\tseq = tokens[i-length:i]\n",
        "\t# convert into a line\n",
        "\tline = ' '.join(seq)\n",
        "\t# store\n",
        "\tsequences.append(line)\n",
        "print('Total Sequences: %d' % len(sequences))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Sequences: 28548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2QfL1C1Xcw6"
      },
      "source": [
        "# save sequences to file\n",
        "out_filename = 'republic_sequences_0.txt'\n",
        "save_doc(sequences, out_filename)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbTygcR5Xcw7"
      },
      "source": [
        "# load\n",
        "in_filename = 'republic_sequences_0.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLGw3GqKXcw8"
      },
      "source": [
        "# integer encode sequences of words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSAsFck2Xcw-"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlBJMhZaXcw_"
      },
      "source": [
        "# separate into input and output\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBMu308vXcxB",
        "outputId": "9e36b269-38e6-4af0-ab5e-b1e117ec63da"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 50, 50)            287450    \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 50, 100)           60400     \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 5749)              580649    \n",
            "=================================================================\n",
            "Total params: 1,018,999\n",
            "Trainable params: 1,018,999\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlTn9NvtXcxC",
        "outputId": "6becd932-bead-49f0-e2b8-b27fc1c600cb"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "model.fit(X, y, batch_size=128, epochs=100)\n",
        " \n",
        "# save the model to file\n",
        "model.save('model_0.h5')"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "224/224 [==============================] - 14s 45ms/step - loss: 8.1601 - accuracy: 0.0079\n",
            "Epoch 2/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 7.6442 - accuracy: 0.0088\n",
            "Epoch 3/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 7.5559 - accuracy: 0.0100\n",
            "Epoch 4/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 7.4841 - accuracy: 0.0094\n",
            "Epoch 5/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 7.3979 - accuracy: 0.0099\n",
            "Epoch 6/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 7.3596 - accuracy: 0.0097\n",
            "Epoch 7/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 7.2856 - accuracy: 0.0104\n",
            "Epoch 8/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 7.2015 - accuracy: 0.0111\n",
            "Epoch 9/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 7.0918 - accuracy: 0.0131\n",
            "Epoch 10/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 6.9962 - accuracy: 0.0114\n",
            "Epoch 11/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 6.8903 - accuracy: 0.0141\n",
            "Epoch 12/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 6.8001 - accuracy: 0.0141\n",
            "Epoch 13/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 6.6734 - accuracy: 0.0160\n",
            "Epoch 14/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 6.5740 - accuracy: 0.0158\n",
            "Epoch 15/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 6.4751 - accuracy: 0.0190\n",
            "Epoch 16/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 6.3489 - accuracy: 0.0200\n",
            "Epoch 17/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 6.2714 - accuracy: 0.0231\n",
            "Epoch 18/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 6.1549 - accuracy: 0.0228\n",
            "Epoch 19/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 6.0448 - accuracy: 0.0245\n",
            "Epoch 20/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 5.9379 - accuracy: 0.0265\n",
            "Epoch 21/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 5.8191 - accuracy: 0.0304\n",
            "Epoch 22/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 5.7239 - accuracy: 0.0341\n",
            "Epoch 23/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 5.6246 - accuracy: 0.0393\n",
            "Epoch 24/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 5.5001 - accuracy: 0.0446\n",
            "Epoch 25/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 5.3898 - accuracy: 0.0529\n",
            "Epoch 26/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 5.2698 - accuracy: 0.0634\n",
            "Epoch 27/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 5.1546 - accuracy: 0.0736\n",
            "Epoch 28/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 5.0662 - accuracy: 0.0852\n",
            "Epoch 29/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 4.9588 - accuracy: 0.0969\n",
            "Epoch 30/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 4.8832 - accuracy: 0.1061\n",
            "Epoch 31/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 4.7568 - accuracy: 0.1186\n",
            "Epoch 32/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 4.6835 - accuracy: 0.1301\n",
            "Epoch 33/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 4.6172 - accuracy: 0.1371\n",
            "Epoch 34/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 4.5149 - accuracy: 0.1491\n",
            "Epoch 35/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 4.4174 - accuracy: 0.1615\n",
            "Epoch 36/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 4.3534 - accuracy: 0.1720\n",
            "Epoch 37/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 4.2300 - accuracy: 0.1908\n",
            "Epoch 38/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 4.1743 - accuracy: 0.1979\n",
            "Epoch 39/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 4.0677 - accuracy: 0.2161\n",
            "Epoch 40/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 3.9781 - accuracy: 0.2286\n",
            "Epoch 41/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 3.8950 - accuracy: 0.2393\n",
            "Epoch 42/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 3.8364 - accuracy: 0.2471\n",
            "Epoch 43/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 3.7552 - accuracy: 0.2623\n",
            "Epoch 44/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 3.6749 - accuracy: 0.2701\n",
            "Epoch 45/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 3.6279 - accuracy: 0.2755\n",
            "Epoch 46/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 3.6171 - accuracy: 0.2793\n",
            "Epoch 47/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 3.5160 - accuracy: 0.2958\n",
            "Epoch 48/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 3.4459 - accuracy: 0.3099\n",
            "Epoch 49/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 3.3801 - accuracy: 0.3156\n",
            "Epoch 50/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 3.3107 - accuracy: 0.3302\n",
            "Epoch 51/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 3.2681 - accuracy: 0.3341\n",
            "Epoch 52/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 3.2232 - accuracy: 0.3428\n",
            "Epoch 53/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 3.1857 - accuracy: 0.3477\n",
            "Epoch 54/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 3.3189 - accuracy: 0.3257\n",
            "Epoch 55/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 3.0842 - accuracy: 0.3656\n",
            "Epoch 56/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 3.0868 - accuracy: 0.3616\n",
            "Epoch 57/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 2.9626 - accuracy: 0.3910\n",
            "Epoch 58/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 2.9585 - accuracy: 0.3882\n",
            "Epoch 59/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 2.9509 - accuracy: 0.3839\n",
            "Epoch 60/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 2.8808 - accuracy: 0.3987\n",
            "Epoch 61/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 2.8548 - accuracy: 0.4048\n",
            "Epoch 62/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 2.8086 - accuracy: 0.4116\n",
            "Epoch 63/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 2.7262 - accuracy: 0.4258\n",
            "Epoch 64/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 2.7211 - accuracy: 0.4272\n",
            "Epoch 65/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 2.6361 - accuracy: 0.4454\n",
            "Epoch 66/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 2.6481 - accuracy: 0.4395\n",
            "Epoch 67/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 2.5831 - accuracy: 0.4534\n",
            "Epoch 68/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 2.5508 - accuracy: 0.4582\n",
            "Epoch 69/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 2.5666 - accuracy: 0.4533\n",
            "Epoch 70/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 2.5174 - accuracy: 0.4598\n",
            "Epoch 71/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 2.4792 - accuracy: 0.4726\n",
            "Epoch 72/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 2.4429 - accuracy: 0.4731\n",
            "Epoch 73/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 2.3577 - accuracy: 0.4978\n",
            "Epoch 74/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 2.3686 - accuracy: 0.4881\n",
            "Epoch 75/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 2.3034 - accuracy: 0.5036\n",
            "Epoch 76/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 2.2815 - accuracy: 0.5062\n",
            "Epoch 77/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 2.2508 - accuracy: 0.5140\n",
            "Epoch 78/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 2.2501 - accuracy: 0.5113\n",
            "Epoch 79/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 2.2245 - accuracy: 0.5151\n",
            "Epoch 80/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 2.1796 - accuracy: 0.5218\n",
            "Epoch 81/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 2.2101 - accuracy: 0.5167\n",
            "Epoch 82/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 2.1283 - accuracy: 0.5316\n",
            "Epoch 83/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 2.1120 - accuracy: 0.5350\n",
            "Epoch 84/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 2.1105 - accuracy: 0.5404\n",
            "Epoch 85/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 2.1495 - accuracy: 0.5254\n",
            "Epoch 86/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 1.9793 - accuracy: 0.5664\n",
            "Epoch 87/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 1.9352 - accuracy: 0.5720\n",
            "Epoch 88/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 1.9419 - accuracy: 0.5700\n",
            "Epoch 89/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 1.9317 - accuracy: 0.5719\n",
            "Epoch 90/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 1.8929 - accuracy: 0.5808\n",
            "Epoch 91/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 1.8725 - accuracy: 0.5861\n",
            "Epoch 92/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 1.8928 - accuracy: 0.5813\n",
            "Epoch 93/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 1.8324 - accuracy: 0.5916\n",
            "Epoch 94/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 1.8222 - accuracy: 0.5959\n",
            "Epoch 95/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 1.8093 - accuracy: 0.5924\n",
            "Epoch 96/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 1.7522 - accuracy: 0.6049\n",
            "Epoch 97/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 1.7188 - accuracy: 0.6141\n",
            "Epoch 98/100\n",
            "224/224 [==============================] - 10s 43ms/step - loss: 1.7214 - accuracy: 0.6176\n",
            "Epoch 99/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 1.6843 - accuracy: 0.6235\n",
            "Epoch 100/100\n",
            "224/224 [==============================] - 10s 44ms/step - loss: 1.6745 - accuracy: 0.6249\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfL5PnZsXcxD"
      },
      "source": [
        "dump(tokenizer, open('tokenizer_0.pkl', 'wb'))"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE-a0IjDXcxE",
        "outputId": "9bf59874-127a-42d0-9e2e-c825ef946926"
      },
      "source": [
        "# load cleaned text sequences\n",
        "in_filename = 'republic_sequences_0.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')\n",
        "seq_length = len(lines[0].split()) - 1\n",
        "\n",
        "# load the model\n",
        "model = load_model('model_0.h5')\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('tokenizer_0.pkl', 'rb'))\n",
        "\n",
        "# select a seed text\n",
        "seed_text = lines[randint(0,len(lines))]\n",
        "# seed_text = \"for being productive\"\n",
        "print(seed_text + '\\n')\n",
        "\n",
        "# generate new text\n",
        "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
        "\n",
        "# print(generated)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "today book architecture many dark century vitruvius work forgotten early 1400s many piece classical writing including lucretius epic poem nature thing cicero oration rediscovered collected pioneering italian humanist poggio bracciolini monastery switzerland poggio found eighthcentury copy vitruvius opus sent back florence became part firmament rediscovered classical work birthed renaissance brunelleschi used\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nG4BVfzkXcxH",
        "outputId": "b5d472c9-6267-405b-cc4f-5eb84d5b9657"
      },
      "source": [
        "print(seed_text+\" \"+generated)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "today book architecture many dark century vitruvius work forgotten early 1400s many piece classical writing including lucretius epic poem nature thing cicero oration rediscovered collected pioneering italian humanist poggio bracciolini monastery switzerland poggio found eighthcentury copy vitruvius opus sent back florence became part firmament rediscovered classical work birthed renaissance brunelleschi used reference traveled rome young measure year timeless name experiment flapping room drawing floor gallerie bracciolini monastery switzerland euler vitruvius opus hand penis membro virile bracciolini latin height court notebook entry 1980s andrea drawing rediscovered architectural historian claudio sgarbi found eighthcentury hand leonardo elbow armpit oneeighth height breast head quarter leonardo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNYGRS4SDr5X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}