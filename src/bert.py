# -*- coding: utf-8 -*-
"""BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YKAOsy35OaXOCD_0jbhf73PmWV2s60jX
"""

# !pip install transformers
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import transformers
from transformers import AutoModel, BertTokenizerFast
from transformers import BertForSequenceClassification
from transformers import Trainer, TrainingArguments

combined_data = pd.read_csv('combined_data.csv')
# combined_data = pd.read_csv('combined_data_sentence_broken.csv')
combined_data.head()

tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased", do_lower_case=True)

train_text, valid_text, train_labels, val_labels = train_test_split(combined_data['text'].tolist(), combined_data['class'].tolist(), 
                                                                    test_size=0.2)

# tokenize the dataset, truncate when passed `max_length`, 
# and pad with 0's when less than `max_length`
max_length = 512
train_encodings = tokenizer(train_text, truncation=True, padding=True, max_length=max_length)
val_encodings = tokenizer(valid_text, truncation=True, padding=True, max_length=max_length)

class myDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = myDataset(train_encodings, train_labels)
val_dataset = myDataset(val_encodings, val_labels)

model = BertForSequenceClassification.from_pretrained("bert-base-uncased")

training_args = TrainingArguments(
    output_dir='./results',          # output directory
    evaluation_strategy="epoch",     # Evaluation is done at the end of each epoch.
    num_train_epochs=20,              # total number of training epochs
    per_device_train_batch_size=8,  # batch size per device during training
    per_device_eval_batch_size=8,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.03,               # strength of weight decay
    save_total_limit=1,              # limit the total amount of checkpoints. Deletes the older checkpoints.    
)


trainer = Trainer(
    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset,             # evaluation dataset
)

trainer.train()

trainer.evaluate()

model.save_pretrained("/content/gdrive/MyDrive/models/bert_classification_lm")

# model.from_pretrained("/content/gdrive/MyDrive/models/bert_classification_lm")

# !zip "content/bert_classification_lm.zip" "content/gdrive/My Drive/models/bert_classification_lm"
!cp "/content/gdrive/MyDrive/models/bert_classification_lm" "content/gdrive/My Drive/models"

