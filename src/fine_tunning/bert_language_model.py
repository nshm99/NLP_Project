# -*- coding: utf-8 -*-
"""BERT_language_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11ba3YoX2gduu4Fg39CUYohMxty85Gi32
"""

!pip install transformers==4.2.2

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.model_selection import train_test_split
# from sklearn.metrics import classification_report
import transformers
# from transformers import AutoModel, BertTokenizerFast
# from transformers import BertForSequenceClassification
from transformers import Trainer, TrainingArguments
from transformers import AutoTokenizer, AutoModelWithLMHead
from transformers import TextDataset,DataCollatorForLanguageModeling

def build_text_files(texts, dest_path):
    f = open(dest_path, 'w')
    data = ''
    for text in texts:
        data += text + "  "
    f.write(data)

def load_dataset(train_path,test_path,tokenizer):
    train_dataset = TextDataset(
          tokenizer=tokenizer,
          file_path=train_path,
          block_size=128)
     
    test_dataset = TextDataset(
          tokenizer=tokenizer,
          file_path=test_path,
          block_size=128)   
    
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, mlm=False,
    )
    return train_dataset,test_dataset,data_collator

combined_data = pd.read_csv('combined_data.csv')
# combined_data = pd.read_csv('combined_data_sentence_broken.csv')
combined_data.head()

"""Motivational

"""

combined_data = combined_data[(combined_data['class']==0)]

train_text, valid_text, train_labels, val_labels = train_test_split(combined_data['text'].tolist(), combined_data['class'].tolist(), 
                                                                    test_size=0.15)
train_path = 'train_dataset.txt'
test_path = 'test_dataset.txt'

build_text_files(train_text, train_path)
build_text_files(valid_text, test_path)

# tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased", do_lower_case=True)
tokenizer = AutoTokenizer.from_pretrained("distilgpt2")

train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)

model = AutoModelWithLMHead.from_pretrained("distilgpt2")

training_args = TrainingArguments(
    output_dir='./motivational.bert_lm',          
    overwrite_output_dir=True, 
    num_train_epochs=3,
    per_device_train_batch_size=32, 
    per_device_eval_batch_size=64,  
    eval_steps = 400, 
    save_steps=800, 
    warmup_steps=500,
    prediction_loss_only=True,
)


trainer = Trainer(
    model=model,  
    args=training_args,
    train_dataset=train_dataset,   
    eval_dataset=test_dataset,     
    data_collator=data_collator,
)

trainer.train()

trainer.evaluate()

# model.save_pretrained("/content/gdrive/MyDrive/models/bert_classification_lm")
trainer.save_model()

from transformers import pipeline

pipline = pipeline('text-generation',model='./motivational.bert_lm', tokenizer='distilgpt2',config={'max_length':800})

pipline('being productive is to')[0]['generated_text']

from google.colab import drive
drive.mount('/content/drive')

!zip "/content/motivational.bert_lm.zip" "/content/motivational.bert_lm"
!cp "/content/motivational.bert_lm.zip" "/content/drive/MyDrive"

"""non Motivational"""

combined_data = pd.read_csv('combined_data.csv')
combined_data.head()

combined_data = combined_data[(combined_data['class'] == 1)]

train_text, valid_text, train_labels, val_labels = train_test_split(combined_data['text'].tolist(), combined_data['class'].tolist(), 
                                                                    test_size=0.15)
train_path = 'train_dataset.txt'
test_path = 'test_dataset.txt'

build_text_files(train_text, train_path)
build_text_files(valid_text, test_path)

train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)

training_args = TrainingArguments(
    output_dir='./nonMotivational.bert_lm',          
    overwrite_output_dir=True, 
    num_train_epochs=3,
    per_device_train_batch_size=32, 
    per_device_eval_batch_size=64,  
    eval_steps = 400, 
    save_steps=800, 
    warmup_steps=500,
    prediction_loss_only=True,
)


trainer = Trainer(
    model=model,                    
    args=training_args,             
    train_dataset=train_dataset,    
    eval_dataset=test_dataset,      
    data_collator=data_collator,
)

trainer.train()

trainer.evaluate()

# model.save_pretrained("/content/gdrive/MyDrive/models/bert_classification_lm")
trainer.save_model()

from transformers import pipeline

pipline = pipeline('text-generation',model='./nonMotivational.bert_lm', tokenizer='distilgpt2',config={'max_length':800})

pipline('politic is')[0]['generated_text']

!zip "/content/nonMotivational.bert_lm.zip" "/content/nonMotivational.bert_lm"
!cp "/content/nonMotivational.bert_lm.zip" "/content/drive/MyDrive"

